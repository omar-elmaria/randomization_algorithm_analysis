{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_storage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pingouin as pg\n",
    "import subprocess\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "import uuid\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define some input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_window_size = [2, 3, 4] # 2, 3, and 4 hours\n",
    "num_variants = [2, 3, 4, 5, 6, 7] # 2, 3, 4, 5, 6, and 7 variants\n",
    "exp_length = [7, 14, 21, 28] # 7, 14, 21, and 28 days\n",
    "col_list = [\n",
    "    'actual_df_paid_by_customer', 'gfv_local', 'gmv_local', 'commission_local', 'joker_vendor_fee_local', # Customer KPIs (1)\n",
    "    'sof_local', 'service_fee_local', 'revenue_local', 'delivery_costs_local', 'gross_profit_local', # Customer KPIs (2)\n",
    "    'dps_mean_delay', 'delivery_distance_m', 'actual_DT' # Logistics KPIs\n",
    "]\n",
    "entity_asa_zone_dict = [ # Define a list of dictionaries containing the entity IDs, ASA IDs, and zone names that will be used in the analysis\n",
    "    # SG\n",
    "    {\"entity_id\": \"FP_SG\", \"asa_id\": 559, \"zone_names\": [\"Bukitpanjang\", \"Jurongwest\", \"Woodlands\"], \"zone_group_identifier\": \"zg_1\"},\n",
    "    {\"entity_id\": \"FP_SG\", \"asa_id\": 560, \"zone_names\": [\"Far_east\", \"Jurong east\"], \"zone_group_identifier\": \"zg_2\"},\n",
    "\n",
    "    # HK\n",
    "    {\"entity_id\": \"FP_HK\", \"asa_id\": 402, \"zone_names\": [\"To kwa wan rider\", \"Kowloon city rider\", \"Lai chi kok rider\"], \"zone_group_identifier\": \"zg_3\"},\n",
    "    {\"entity_id\": \"FP_HK\", \"asa_id\": 406, \"zone_names\": [\"Ma liu shui rider\", \"Kwai chung rider\", \"Sai kung rider\", \"Sheung shui rider\", \"Tai po rider\", \"Tai wai rider\", \"Tin shui wai rider\", \"Tsing yi rider\", \"Tsuen wan rider\", \"Tuen mun rider\", \"Tun chung rider\", \"Yuen long rider\"], \"zone_group_identifier\": \"zg_4\"},\n",
    "    {\"entity_id\": \"FP_HK\", \"asa_id\": 398, \"zone_names\": [\"Admiralty cwb rider\", \"Happy valley cwb rider\", \"Kennedy town rider\", \"Quarry bay rider\"], \"zone_group_identifier\": \"zg_5\"},\n",
    "\n",
    "    # PH\n",
    "    {\"entity_id\": \"FP_PH\", \"asa_id\": 496, \"zone_names\": [\"South alabang atc\", \"Paranaque\", \"North Ias pinas\", \"North alabang atc\", \"Bf homes\"], \"zone_group_identifier\": \"zg_6\"},\n",
    "    {\"entity_id\": \"FP_PH\", \"asa_id\": 525, \"zone_names\": [\"Bacoor north\", \"Tagaytay\", \"Dasmarinas\", \"Imus\"], \"zone_group_identifier\": \"zg_7\"},\n",
    "    {\"entity_id\": \"FP_PH\", \"asa_id\": 528, \"zone_names\": [\"Antipolo north\", \"Malabon\", \"Sjdm\", \"Valenzuela\"], \"zone_group_identifier\": \"zg_8\"},\n",
    "    {\"entity_id\": \"FP_PH\", \"asa_id\": 508, \"zone_names\": [\"Makati\", \"Pasay\"], \"zone_group_identifier\": \"zg_9\"}\n",
    "]\n",
    "zone_groups = [i[\"zone_group_identifier\"] for i in entity_asa_zone_dict]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Instantiate a BQ client and run the SQL query that pulls the historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client(project=\"logistics-data-staging-flat\")\n",
    "bqstorage_client = bigquery_storage.BigQueryReadClient()\n",
    "\n",
    "with open(\"sql_queries.sql\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    query = f.read()\n",
    "    f.close()\n",
    "\n",
    "# client.query(query=query).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1422904/1422904 [00:31<00:00, 45667.89rows/s]\n"
     ]
    }
   ],
   "source": [
    "# Pull the data from the final table generated by the query\n",
    "df = client.query(\"\"\"SELECT * FROM `dh-logistics-product-ops.pricing.ab_test_individual_orders_augmented_randomization_algo_analysis`\"\"\")\\\n",
    "    .result()\\\n",
    "    .to_dataframe(bqstorage_client=bqstorage_client, progress_bar_type=\"tqdm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create a new data frame with the combinations stipulated in the dictionary above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = []\n",
    "for i in entity_asa_zone_dict:\n",
    "    df_iter = df[(df[\"entity_id\"] == i[\"entity_id\"]) & (df[\"asa_id\"] == i[\"asa_id\"]) & (df[\"zone_name\"].isin(i[\"zone_names\"]))]\n",
    "    df_iter[\"zone_group_identifier\"] = i[\"zone_group_identifier\"]\n",
    "    df_reduced.append(df_iter)\n",
    "\n",
    "# Convert df_reduced to a dataframe\n",
    "df_reduced = pd.concat(df_reduced)\n",
    "\n",
    "# Add a new field to df_reduced showing a different format of \"dps_sessionid_created_at_utc\". We want to display the format followed by DPS, which is \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "df_reduced[\"dps_sessionid_created_at_utc_formatted\"] = df_reduced[\"dps_sessionid_created_at_utc\"]\\\n",
    "    .apply(lambda x: pd.to_datetime(dt.datetime.strftime(x, \"%Y-%m-%dT%H:%M:%SZ\")))\n",
    "\n",
    "df_reduced.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: The shell script that runs the randomization algorithm needs the starting time of the experiment as one of its input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define that as the minimum dps_session_start_timestamp per zone_group_identifier\n",
    "df_min_max_dps_session_start_ts = df_reduced.groupby([\"entity_id\", \"zone_group_identifier\"])[\"dps_sessionid_created_at_utc_formatted\"]\\\n",
    "    .agg([\"min\", \"max\"])\\\n",
    "    .reset_index()\\\n",
    "    .rename(columns={\"min\": \"min_dps_session_start_ts\", \"max\": \"max_dps_session_start_ts\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.1: Create a function that takes the zone_group_identifier and creates a CSV file called input_{zg_identifier}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that takes the zone_group_identifier and creates a CSV file called input_{zg_identifier}\n",
    "# This file contains the details necessary to run the randomization algorithm\n",
    "def input_csv_func(zg_identifier):\n",
    "    df_stg = df_reduced[df_reduced[\"zone_group_identifier\"] == zg_identifier][[\"platform_order_code\", \"zone_id\", \"dps_sessionid_created_at_utc_formatted\"]]\\\n",
    "        .sort_values(\"dps_sessionid_created_at_utc_formatted\")\\\n",
    "        .reset_index(drop=True)\n",
    "    df_stg[\"dps_sessionid_created_at_utc_formatted\"] = df_stg[\"dps_sessionid_created_at_utc_formatted\"].apply(lambda x: str(x))\n",
    "    df_stg.to_csv(f\"input.csv\", index=False, header=False, date_format=\"str\")\n",
    "\n",
    "# Invoke the function that creates the input file. Keep in mind that this overwrites the already existing input.csv file\n",
    "input_csv_func(zg_identifier=zone_groups[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.2: Convert the CSV file to UNIX format and run the variant allocation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dos2unix: converting file input.csv to Unix format...\n"
     ]
    }
   ],
   "source": [
    "%%script \"C:/Program Files/Git/bin/bash.exe\"\n",
    "dos2unix input.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: 28115\n",
      "salt: DB0720FD-326E-407F-9EA2-512BF8154DDE\n",
      "Switchback parameters are valid, starting experiment..\n",
      "Allocation is complete. Results are available in output.csv file\n"
     ]
    }
   ],
   "source": [
    "%%script \"C:/Program Files/Git/bin/bash.exe\"\n",
    "./run-allocation.sh -w 3.0 -v 3 -t 2023-01-01T00:48:04Z -k 28115 -s DB0720FD-326E-407F-9EA2-512BF8154DDE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Create a function that gives a random UUID to each time interval. Note: This part will be removed once the UUID functionality is incorporated in the JS function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hr_interval_date_func_random(zg_id, test_length, sb_interval, zone_name_list: list):\n",
    "    min_timestamp_zg_id = df_min_max_dps_session_start_ts[df_min_max_dps_session_start_ts[\"zone_group_identifier\"] == zg_id].reset_index()[\"min_dps_session_start_ts\"][0]\n",
    "    num_time_units = int((24 / sb_interval) * test_length)\n",
    "\n",
    "    # Create an array of timestamps separated by the switchback window size\n",
    "    df_mapping = [min_timestamp_zg_id] # Decalre teh df_mapping variable as a list with the first value being min_timestamp_zg_id\n",
    "    timestamp_iter = min_timestamp_zg_id # Initialize the timestamp_iter with min_timestamp_zg_id\n",
    "    for i in range(1, num_time_units):\n",
    "        df_mapping.append(timestamp_iter + timedelta(hours = 3))\n",
    "        timestamp_iter = timestamp_iter + timedelta(hours=3) # Update the \n",
    "    df_mapping = pd.DataFrame(df_mapping, columns=[\"dps_session_created_at\"]) # Convert the list to a data frame\n",
    "\n",
    "    # Create new columns\n",
    "    df_mapping[\"dps_session_created_date\"] = df_mapping[\"dps_session_created_at\"].apply(lambda x: pd.to_datetime(x.date()))\n",
    "    df_mapping[\"dps_session_created_at_interval\"] = pd.cut(df_mapping[\"dps_session_created_at\"], bins=num_time_units, right=False)\n",
    "    df_mapping[\"common_key\"] = 0\n",
    "\n",
    "    # Create a new data frame containing the zones in the zone group ID\n",
    "    df_zone_id = pd.DataFrame({\"zone_name\": zone_name_list, \"common_key\": 0})\n",
    "    df_mapping = pd.merge(left=df_mapping, right=df_zone_id, how=\"outer\", on=\"common_key\")\n",
    "    df_mapping.drop(\"common_key\", axis=1, inplace=True)\n",
    "    \n",
    "    rnd_id_list = [] # Create the full list that the rng.choice would choose from\n",
    "    for i in range(1, len(df_mapping) + 1):\n",
    "        rnd_id_list.append(uuid.uuid4())\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "    df_mapping['time_zone_unit_id'] = rng.choice(rnd_id_list, replace = False, axis = 0, size = len(df_mapping))\n",
    "    return df_mapping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Create a function that gets the p-value for one simulation run. One simulation run entails one zg_id, sb_window_size, number_of_variants, and experiment length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that gets the p-value for one simulation run. One simulation run entails one zg_id, sb_window_size, number_of_variants, and experiment length\n",
    "def p_val_func(zg_id, exp_length, sb_window_size):\n",
    "    # After the output.csv file is created, retrieve the variants from the output.csv file and join them to df_reduced\n",
    "    df_variants = pd.read_csv(\"output.csv\")\n",
    "    df_analysis = df_reduced[df_reduced[\"zone_group_identifier\"] == zg_id].copy() # Create a copy of df_reduced just for the zg_id being analysed\n",
    "    df_analysis = pd.merge(left=df_analysis, right=df_variants, how=\"left\", left_on=\"platform_order_code\", right_on=\"OrderID\")\n",
    "    df_analysis.drop(\"OrderID\", axis=1, inplace=True)\n",
    "\n",
    "    ##-----------------------------------------------------SEPARATOR-----------------------------------------------------##\n",
    "\n",
    "    # Add a column indicating the week number\n",
    "    df_analysis[\"dps_session_created_date\"] = df_analysis[\"dps_sessionid_created_at_utc_formatted\"].apply(lambda x: pd.to_datetime(datetime.date(x)))\n",
    "    # Change the KPI columns to numeric\n",
    "    df_analysis[col_list] = df_analysis[col_list].apply(lambda x: pd.to_numeric(x))\n",
    "\n",
    "    ##-----------------------------------------------------SEPARATOR-----------------------------------------------------##\n",
    "\n",
    "    # Create a conditions list\n",
    "    start_date = df_analysis[\"created_date_local\"].min()\n",
    "    conditions = [\n",
    "        (df_analysis[\"created_date_local\"] >= start_date) & (df_analysis[\"created_date_local\"] <= start_date + timedelta(6)),\n",
    "        (df_analysis[\"created_date_local\"] >= start_date + timedelta(7)) & (df_analysis[\"created_date_local\"] <= start_date + timedelta(13)),\n",
    "        (df_analysis[\"created_date_local\"] >= start_date + timedelta(14)) & (df_analysis[\"created_date_local\"] <= start_date + timedelta(20)),\n",
    "        (df_analysis[\"created_date_local\"] >= start_date + timedelta(21)) & (df_analysis[\"created_date_local\"] <= start_date + timedelta(27)),\n",
    "    ]\n",
    "\n",
    "    df_analysis[\"week_num\"] = np.select(condlist=conditions, choicelist=[\"week_1\", \"week_2\", \"week_3\", \"week_4\"])\n",
    "\n",
    "    ##-----------------------------------------------------SEPARATOR-----------------------------------------------------##\n",
    "\n",
    "    # Create the data frame containing the random UUIDs for each time interval\n",
    "    df_mapping = hr_interval_date_func_random(zg_id=zg_id, test_length=exp_length, sb_interval=sb_window_size, zone_name_list=df_analysis[\"zone_name\"].unique())\n",
    "\n",
    "    # Create a function that returns the right hr_interval from df_mapping for any given number\n",
    "    def check_right_interval(num, col):\n",
    "        for i in col:\n",
    "            if num in i:\n",
    "                return i\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    # Get the right interval using the \"check_right_interval\" function\n",
    "    df_analysis['dps_session_created_at_interval'] = df_analysis\\\n",
    "        .apply(lambda x: check_right_interval(x['dps_sessionid_created_at_utc_formatted'], df_mapping['dps_session_created_at_interval']), axis = 1)\n",
    "\n",
    "    # Filter df_analysis based on exp_length\n",
    "    df_analysis = df_analysis[df_analysis[\"day_num\"] <= exp_length]\n",
    "\n",
    "    # Merge the random UUIDs with df_analysis. Note: This part will be removed once the UUID functionality is incorporated in the JS function\n",
    "    df_analysis = pd.merge(left = df_analysis, right = df_mapping, how = 'left', on = [\"dps_session_created_at_interval\", \"zone_name\", \"dps_session_created_date\"])\n",
    "\n",
    "    ##-----------------------------------------------------SEPARATOR-----------------------------------------------------##\n",
    "\n",
    "    # Calculate the \"total\" metrics and rename the column label to \"df_per_order_metrics\"\n",
    "    df_analysis_tot = round(df_analysis.groupby([\"time_zone_unit_id\", \"Variant\"])[col_list[:-3]].sum(), 2)\n",
    "    df_analysis_tot['order_count'] = df_analysis.groupby([\"time_zone_unit_id\", \"Variant\"])['platform_order_code'].nunique()\n",
    "    df_analysis_tot = df_analysis_tot.rename_axis(['df_tot_metrics'], axis = 1)\n",
    "\n",
    "    # Calculate the \"total\" metrics and rename the column label to \"df_per_order_metrics\"\n",
    "    df_analysis_per_order_cust_kpis = df_analysis_tot.copy()\n",
    "\n",
    "    for iter_col in df_analysis_per_order_cust_kpis.columns[:-1]:\n",
    "        df_analysis_per_order_cust_kpis[iter_col] = round(df_analysis_per_order_cust_kpis[iter_col] / df_analysis_per_order_cust_kpis['order_count'], 4)\n",
    "\n",
    "    df_analysis_per_order_log_kpis = round(df_analysis.groupby([\"time_zone_unit_id\", \"Variant\"])[col_list[-3:]].mean(), 2) \n",
    "    df_analysis_per_order = pd.concat([df_analysis_per_order_cust_kpis, df_analysis_per_order_log_kpis], axis = 1)\n",
    "    df_analysis_per_order = df_analysis_per_order.rename_axis(['df_per_order_metrics'], axis = 1)\n",
    "\n",
    "    # Reset the indices of the \n",
    "    df_analysis_tot = df_analysis_tot.reset_index()\n",
    "    df_analysis_per_order = df_analysis_per_order.reset_index()\n",
    "\n",
    "    return df_analysis, df_analysis_tot, df_analysis_per_order"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_sb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14e010e4cd1c1ecfc2a757c09121a44deab645fe879881bec23ed2eed3f5394d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
