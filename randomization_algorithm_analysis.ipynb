{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import bigquery_storage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "from math import ceil\n",
    "import uuid\n",
    "import scipy.stats\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define some input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a BQ client and run the SQL query that pulls the historical data\n",
    "client = bigquery.Client(project=\"logistics-data-staging-flat\")\n",
    "bqstorage_client = bigquery_storage.BigQueryReadClient()\n",
    "\n",
    "with open(\"sql_queries.sql\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    query = f.read()\n",
    "    f.close()\n",
    "\n",
    "client.query(query=query).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1422904/1422904 [01:07<00:00, 21182.12rows/s]\n"
     ]
    }
   ],
   "source": [
    "# Pull the data from the final table generated by the query\n",
    "df = client.query(\"\"\"SELECT * FROM `dh-logistics-product-ops.pricing.ab_test_individual_orders_augmented_randomization_algo_analysis`\"\"\").result().to_dataframe(bqstorage_client=bqstorage_client, progress_bar_type=\"tqdm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of dictionaries containing the entity IDs, ASA IDs, and zone names that will be used in the analysis\n",
    "entity_asa_zone_dict = [\n",
    "    # SG\n",
    "    {\"entity_id\": \"FP_SG\", \"asa_id\": 559, \"zone_names\": [\"Bukitpanjang\", \"Jurongwest\", \"Woodlands\"], \"zone_group_identifier\": \"zg_1\"},\n",
    "    {\"entity_id\": \"FP_SG\", \"asa_id\": 560, \"zone_names\": [\"Far_east\", \"Jurong east\"], \"zone_group_identifier\": \"zg_2\"},\n",
    "\n",
    "    # HK\n",
    "    {\"entity_id\": \"FP_HK\", \"asa_id\": 402, \"zone_names\": [\"To kwa wan rider\", \"Kowloon city rider\", \"Lai chi kok rider\"], \"zone_group_identifier\": \"zg_3\"},\n",
    "    {\"entity_id\": \"FP_HK\", \"asa_id\": 406, \"zone_names\": [\"Ma liu shui rider\", \"Kwai chung rider\", \"Sai kung rider\", \"Sheung shui rider\", \"Tai po rider\", \"Tai wai rider\", \"Tin shui wai rider\", \"Tsing yi rider\", \"Tsuen wan rider\", \"Tuen mun rider\", \"Tun chung rider\", \"Yuen long rider\"], \"zone_group_identifier\": \"zg_4\"},\n",
    "    {\"entity_id\": \"FP_HK\", \"asa_id\": 398, \"zone_names\": [\"Admiralty cwb rider\", \"Happy valley cwb rider\", \"Kennedy town rider\", \"Quarry bay rider\"], \"zone_group_identifier\": \"zg_5\"},\n",
    "\n",
    "    # PH\n",
    "    {\"entity_id\": \"FP_PH\", \"asa_id\": 496, \"zone_names\": [\"South alabang atc\", \"Paranaque\", \"North Ias pinas\", \"North alabang atc\", \"Bf homes\"], \"zone_group_identifier\": \"zg_6\"},\n",
    "    {\"entity_id\": \"FP_PH\", \"asa_id\": 525, \"zone_names\": [\"Bacoor north\", \"Tagaytay\", \"Dasmarinas\", \"Imus\"], \"zone_group_identifier\": \"zg_7\"},\n",
    "    {\"entity_id\": \"FP_PH\", \"asa_id\": 528, \"zone_names\": [\"Antipolo north\", \"Malabon\", \"Sjdm\", \"Valenzuela\"], \"zone_group_identifier\": \"zg_8\"},\n",
    "    {\"entity_id\": \"FP_PH\", \"asa_id\": 508, \"zone_names\": [\"Makati\", \"Pasay\"], \"zone_group_identifier\": \"zg_9\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new data frame with the combinations stipulated in the dictionary above\n",
    "df_reduced = []\n",
    "for i in entity_asa_zone_dict:\n",
    "    df_iter = df[(df[\"entity_id\"] == i[\"entity_id\"]) & (df[\"asa_id\"] == i[\"asa_id\"]) & (df[\"zone_name\"].isin(i[\"zone_names\"]))]\n",
    "    df_iter[\"zone_group_identifier\"] = i[\"zone_group_identifier\"]\n",
    "    df_reduced.append(df_iter)\n",
    "\n",
    "# Convert df_reduced to a dataframe\n",
    "df_reduced = pd.concat(df_reduced)\n",
    "\n",
    "# Add a new field to df_reduced showing a different format of \"dps_sessionid_created_at_utc\". We want to display the format followed by DPS, which is \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "df_reduced[\"dps_sessionid_created_at_utc_formatted\"] = df_reduced[\"dps_sessionid_created_at_utc\"]\\\n",
    "    .apply(lambda x: pd.to_datetime(dt.datetime.strftime(x, \"%Y-%m-%dT%H:%M:%SZ\")))\n",
    "\n",
    "df_reduced.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shell script that runs the randomization algorithm needs the starting time of the experiment as one of its input\n",
    "# We define that as the minimum dps_session_start_timestamp per zone_group_identifier\n",
    "df_min_max_dps_session_start_ts = df_reduced.groupby([\"entity_id\", \"zone_group_identifier\"])[\"dps_sessionid_created_at_utc_formatted\"]\\\n",
    "    .agg([\"min\", \"max\"])\\\n",
    "    .reset_index()\\\n",
    "    .rename(columns={\"min\": \"min_dps_session_start_ts\", \"max\": \"max_dps_session_start_ts\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that takes the zone_group_identifier and creates a CSV file called input_{zg_identifier}. This file contains the details necessary to run the randomization algorithm\n",
    "def input_csv_func(zg_identifier):\n",
    "    df_stg = df_reduced[df_reduced[\"zone_group_identifier\"] == zg_identifier][[\"platform_order_code\", \"zone_id\", \"dps_sessionid_created_at_utc_formatted\"]]\\\n",
    "        .sort_values(\"dps_sessionid_created_at_utc_formatted\")\\\n",
    "        .reset_index(drop=True)\n",
    "    df_stg[\"dps_sessionid_created_at_utc_formatted\"] = df_stg[\"dps_sessionid_created_at_utc_formatted\"].apply(lambda x: str(x))\n",
    "    df_stg.to_csv(f\"input.csv\", index=False, header=False, date_format=\"str\")\n",
    "\n",
    "# Invoke the function that creates the input file. Keep in mind that this overwrites the already existing input.csv file\n",
    "input_csv_func(zg_identifier=\"zg_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dos2unix: converting file input.csv to Unix format...\n"
     ]
    }
   ],
   "source": [
    "%%script \"C:/Program Files/Git/bin/bash.exe\"\n",
    "dos2unix input.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: 28115\n",
      "salt: DB0720FD-326E-407F-9EA2-512BF8154DDE\n",
      "Switchback parameters are valid, starting experiment..\n",
      "Allocation is complete. Results are available in output.csv file\n"
     ]
    }
   ],
   "source": [
    "%%script \"C:/Program Files/Git/bin/bash.exe\"\n",
    "./run-allocation.sh -w 3.0 -v 3 -t 2023-01-01T00:48:04Z -k 28115 -s DB0720FD-326E-407F-9EA2-512BF8154DDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the output.csv file is created, retrieve the variants from the output.csv file and join them to df_reduced\n",
    "df_variants = pd.read_csv(\"output.csv\")\n",
    "df_analysis = df_reduced[df_reduced[\"zone_group_identifier\"] == \"zg_1\"].copy() # Create a copy of df_reduced just for the zg_id being analysed\n",
    "df_analysis = pd.merge(left=df_analysis, right=df_variants, how=\"left\", left_on=\"platform_order_code\", right_on=\"OrderID\")\n",
    "df_analysis.drop(\"OrderID\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column indicating the week number\n",
    "df_analysis[\"created_date_local\"] = df_analysis[\"order_placed_at_local\"].apply(lambda x: pd.to_datetime(datetime.date(x)))\n",
    "\n",
    "conditions = [\n",
    "    (df_analysis[\"created_date_local\"] >= \"2023-01-01\") & (df_analysis[\"created_date_local\"] <= \"2023-01-07\"),\n",
    "    (df_analysis[\"created_date_local\"] >= \"2023-01-08\") & (df_analysis[\"created_date_local\"] <= \"2023-01-14\"),\n",
    "    (df_analysis[\"created_date_local\"] >= \"2023-01-15\") & (df_analysis[\"created_date_local\"] <= \"2023-01-21\"),\n",
    "    (df_analysis[\"created_date_local\"] >= \"2023-01-22\") & (df_analysis[\"created_date_local\"] <= \"2023-01-28\"),\n",
    "]\n",
    "\n",
    "df_analysis[\"week_num\"] = np.select(condlist=conditions, choicelist=[\"week_1\", \"week_2\", \"week_3\", \"week_4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hr_interval_func_random(sb_interval):\n",
    "    bins = int(24 / sb_interval) # The number of bins by which we will divide the range from 0 to 24. A 2-hour switchback interval will have 12 bin --> [0, 2), [2, 4), [4, 6), ... [22, 24)\n",
    "    if sb_interval >= 1:\n",
    "        end_of_range = 25\n",
    "    elif sb_interval == 0.5:\n",
    "        end_of_range = 24.5\n",
    "    elif sb_interval == 0.25:\n",
    "        end_of_range = 24.25\n",
    "    df_mapping = pd.DataFrame(data = {\n",
    "            'hr_interval': list(pd.cut(np.arange(0, end_of_range, sb_interval), bins = bins, right = False)) # The bins should be closed from the left\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Drop duplicates\n",
    "    df_mapping.drop_duplicates(inplace = True)\n",
    "\n",
    "    unique_intervals = df_mapping['hr_interval'].unique()\n",
    "\n",
    "    rnd_id_list = [] # Create the full list that the rng.choice would choose from\n",
    "    for i in range(1, len(unique_intervals) + 1):\n",
    "        rnd_id_list.append(uuid.uuid4())\n",
    "\n",
    "    rng = np.random.default_rng()\n",
    "    df_mapping['treatment_status_by_time'] = rng.choice(rnd_id_list, replace = False, axis = 0, size = len(df_mapping))\n",
    "    return df_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hr_interval_date_func_random(test_start, test_length, sb_interval):\n",
    "    m = []\n",
    "    date_iter = test_start # Start date of the test in datetime format\n",
    "    for i in range(0, test_length): # The length of a test in days\n",
    "        y = hr_interval_func_random(sb_interval) # The switchback window size\n",
    "        y['sim_run'] = i + 1\n",
    "        y['created_date_local'] = date_iter\n",
    "        date_iter = date_iter + timedelta(days = 1)\n",
    "        m.append(y)\n",
    "\n",
    "    m = pd.concat(m)\n",
    "    m.reset_index(inplace = True, drop = True)\n",
    "    return m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_sb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14e010e4cd1c1ecfc2a757c09121a44deab645fe879881bec23ed2eed3f5394d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
